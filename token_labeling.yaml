bitsandbytes:
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: false
  load_in_4bit: true
lora:
  r : 64
  lora_alpha: 16
  target_modules : ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  lora_dropout: 0.05
  task_type: TOKEN_CLS
training:
  learning_rate : 0.0002
  per_device_train_batch_size : 8
  num_train_epochs : 3
  weight_decay : 0.01
  eval_strategy : 'no'
  save_strategy : 'no'
  lr_scheduler_type : linear
task: token_classification
num_labels: 2
base_model_name: meta-llama/Meta-Llama-3-8B
