{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/redacted/dev/moral-summarization')\n",
    "\n",
    "from moral_summarization.eval import evaluate_CoT_moral_words_predictions\n",
    "\n",
    "\n",
    "results_dir = '../results/final_prompts'\n",
    "#models = ['DeepSeek-R1-Distill-Qwen-32B', 'c4ai-command-r-plus-4bit'] # \n",
    "models = ['Meta-Llama-3-70B-Instruct']\n",
    "article_list = 'articles_in_test_set.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, length_df = evaluate_CoT_moral_words_predictions(\n",
    "    results_dir, models, only_test_set=True, article_list=article_list, seed='345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove last row of length_df\n",
    "length_df = length_df.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "median(length_df['Meta-Llama-3-70B-Instruct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier results in style of CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "from moral_summarization.eval import f1_moral_predictions\n",
    "\n",
    "with open(article_list) as f:\n",
    "    articles_in_test_set = f.read().splitlines()\n",
    "\n",
    "literal_eval_columns = ['predicted_words', 'labeled_words']\n",
    "converters = {column: literal_eval for column in literal_eval_columns}\n",
    "results_class = pd.read_csv('../results/predictions_with_words.csv', converters=converters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_results_df = pd.DataFrame(columns=['predictions', 'labels', 'f1'], index=articles_in_test_set)\n",
    "\n",
    "for article in articles_in_test_set:\n",
    "    article_results = results_class[results_class['article'] == article]\n",
    "\n",
    "    predicted_words = article_results['predicted_words'].to_list()\n",
    "    predicted_words = [word for sublist in predicted_words for word in sublist]\n",
    "    class_results_df.loc[article, 'predictions'] = predicted_words\n",
    "\n",
    "    labeled_words = article_results['labeled_words'].to_list()\n",
    "    labeled_words = [word for sublist in labeled_words for word in sublist]\n",
    "    class_results_df.loc[article, 'labels'] = labeled_words\n",
    "\n",
    "    f1 = f1_moral_predictions(labeled_words, predicted_words)\n",
    "    class_results_df.loc[article, 'f1'] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean to the dataframe\n",
    "class_results_df['f1'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier seqeval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from moral_summarization.metrics import seqeval_metrics\n",
    "\n",
    "class_seqeval_results_df = pd.DataFrame(columns=['f1'], index=articles_in_test_set)\n",
    "\n",
    "for article in articles_in_test_set:\n",
    "    article_results = results_class[results_class['article'] == article]\n",
    "\n",
    "    predictions = article_results['predictions'].to_list()\n",
    "    cleaned_strings = [pred.replace('\\n', '').replace(' ', ', ') for pred in predictions]\n",
    "    class_labels = [ast.literal_eval(cleaned_string) for cleaned_string in cleaned_strings]\n",
    "    predictions = [word for sublist in class_labels for word in sublist]\n",
    "\n",
    "    labels = article_results['labels'].to_list()\n",
    "    cleaned_strings = [pred.replace('\\n', '').replace('    ', ', ') for pred in labels]\n",
    "    class_labels = [ast.literal_eval(cleaned_string) for cleaned_string in cleaned_strings]\n",
    "    labels = [word for sublist in class_labels for word in sublist]\n",
    "\n",
    "    metrics = seqeval_metrics([predictions], [labels])\n",
    "\n",
    "    class_seqeval_results_df.loc[article, 'f1'] = metrics['f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count median of words annotated and predicted by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/redacted/dev/moral-summarization')\n",
    "\n",
    "from moral_summarization.data_utils import *\n",
    "\n",
    "literal_eval_columns = ['predicted_words', 'labeled_words']\n",
    "converters = {column: literal_eval for column in literal_eval_columns}\n",
    "results_class = pd.read_csv('../results/predictions_with_words.csv', converters=converters)\n",
    "\n",
    "from statistics import median\n",
    "\n",
    "article_list = 'articles_in_test_set.txt'\n",
    "with open(article_list, 'r') as f:\n",
    "    articles = f.readlines()\n",
    "articles = [article.strip() for article in articles]\n",
    "\n",
    "# count sum of length of all strings in the column predicted_words\n",
    "count_pred = {article: 0 for article in articles}\n",
    "count_label = {article: 0 for article in articles}\n",
    "for id, row in results_class.iterrows():\n",
    "    count_pred[row['article']] += len(row['predicted_words'])\n",
    "    count_label[row['article']] += len(row['labeled_words'])\n",
    "\n",
    "print(median(count_pred.values()))\n",
    "print(median(count_label.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".summarization-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
