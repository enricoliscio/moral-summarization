{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/redacted/dev/moral-summarization')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from moral_summarization.utils import load_json, load_yaml\n",
    "from moral_summarization.metrics import evaluate_on_df\n",
    "\n",
    "cosine = '/home/redacted/dev/hf-models/hyperparameter_tuning/hyper_token_article_cosine'\n",
    "linear = '/home/redacted/dev/hf-models/hyperparameter_tuning/hyper_token_article_linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_numpy_array(string_data):\n",
    "    # Define a restricted environment for eval\n",
    "    restricted_globals = {\"np\": np, \"__builtins__\": None}\n",
    "    # Use eval to safely evaluate the string as a Python expression\n",
    "    return eval(string_data, restricted_globals)\n",
    "\n",
    "def get_results(hyperparameters_folder, idx_sum=1, process_raw_evaluations=True):\n",
    "    # initialize dataframe to store results\n",
    "    results = pd.DataFrame(columns=['scheduler', 'precision', 'recall', 'f1', 'accuracy'])\n",
    "\n",
    "    configs = {}\n",
    "\n",
    "    # loop through folder of hyperparameters_folder\n",
    "    for hyperparameters_combination in os.listdir(hyperparameters_folder):\n",
    "        folder = os.path.join(hyperparameters_folder, hyperparameters_combination)\n",
    "\n",
    "        precision, recall, f1, accuracy = [], [], [], []\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith('.yaml'):\n",
    "                config = load_yaml(os.path.join(folder, file))\n",
    "                configs[idx_sum+int(hyperparameters_combination)] = config\n",
    "\n",
    "            if process_raw_evaluations is False:\n",
    "                if file.endswith('.json'):\n",
    "                    metrics_fold = load_json(os.path.join(folder, file))\n",
    "                    f1_key = 'f1'\n",
    "                    if 'precision' not in metrics_fold:\n",
    "                        metrics_fold = metrics_fold['macro avg']\n",
    "                        f1_key = 'f1-score'\n",
    "                    precision.append(metrics_fold['precision'])\n",
    "                    recall.append(metrics_fold['recall'])\n",
    "                    f1.append(metrics_fold[f1_key])\n",
    "                #accuracy.append(metrics_fold['accuracy'])\n",
    "\n",
    "            else:\n",
    "                if file.endswith('.csv'):\n",
    "                    fold_results = pd.read_csv(os.path.join(folder, file))\n",
    "                    for column in ['predictions', 'labels']:\n",
    "                        fold_results[column] = fold_results[column].apply(string_to_numpy_array)\n",
    "\n",
    "                    metrics_fold = evaluate_on_df(fold_results, 'token_classification')\n",
    "                    f1_key = 'f1'\n",
    "                    if 'precision' not in metrics_fold:\n",
    "                        metrics_fold = metrics_fold['macro avg']\n",
    "                        f1_key = 'f1-score'\n",
    "                    precision.append(metrics_fold['precision'])\n",
    "                    recall.append(metrics_fold['recall'])\n",
    "                    f1.append(metrics_fold[f1_key])\n",
    "                #accuracy.append(metrics_fold['accuracy'])\n",
    "        \n",
    "        precision = np.mean(precision)\n",
    "        recall = np.mean(recall)\n",
    "        f1 = np.mean(f1)\n",
    "        accuracy = np.mean(accuracy)\n",
    "\n",
    "        results.loc[idx_sum+int(hyperparameters_combination)] = \\\n",
    "            [config['training']['lr_scheduler_type'], precision, recall, f1, accuracy]\n",
    "    \n",
    "    return results, configs\n",
    "\n",
    "def print_best_results(results):\n",
    "    print(\"max F1:\\n\", results.loc[results['f1'].idxmax()])\n",
    "    print('=========================')\n",
    "    # print(\"max accuracy:\\n\", results.loc[results['accuracy'].idxmax()])\n",
    "    # print('=========================')\n",
    "    print(\"max precision:\\n\", results.loc[results['precision'].idxmax()])\n",
    "    print('=========================')\n",
    "    print(\"max recall:\\n\", results.loc[results['recall'].idxmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_linear, configs_linear = get_results(linear, idx_sum=1)\n",
    "results_cosine, configs_cosine = get_results(cosine, idx_sum=20)\n",
    "\n",
    "# concatenate results\n",
    "results = pd.concat([results_linear, results_cosine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_best_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".summarization-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
